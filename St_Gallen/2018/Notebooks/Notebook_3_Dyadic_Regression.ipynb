{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Dyadic Regression\n",
    "#### Econometric Methods for Social Spillovers and Networks\n",
    "#### University of St. Gallen, October 1st to 9th, 2018\n",
    "##### _Bryan S. Graham, UC - Berkeley, bgraham@econ.berkeley.edu_\n",
    "<br>\n",
    "<br>\n",
    "This is the third of a series of iPython Jupyter notebooks designed to accompany a series of instructional lectures given at the St. Gallen University from Oct 1st to Oct 9th, 2018. The scripts below were written for Python 3.6. The Anaconda distribution of Python, available at https://www.continuum.io/downloads, comes bundled with most the scientific computing packages used in these notebooks.\n",
    "<br>\n",
    "<br>\n",
    "For more information about the course please visit my webpage at http://bryangraham.github.io/econometrics/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code citation:\n",
    "Graham, Bryan S. (2018). \"Notebook 3: Dyadic Regression: St. Gallen Social and Economic Networks Course Jupyter Notebook,\" (Version 1.0) [Computer program]. Available at http://bryangraham.github.io/econometrics/ (Accessed 20 September 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyadic regression and the gravity model of international trade\n",
    "\n",
    "Jo√£o Santos Silva and Silvana Tenreyro (2006, _Review of Economics and Statistics_) advocate the application of Poisson regression methods to the analysis of trade flows across countries. Their work has been influential in the empirical trade literature. Google Scholar calls their work a \"classic paper\" in economics (see this [blog post](https://scholar.googleblog.com/2017/06/classic-papers-articles-that-have-stood.html) for more information on Google Scholar \"classic papers\"). An open access copy of the paper can be found [here](https://www.mitpressjournals.org/doi/pdf/10.1162/rest.88.4.641). Replication data and additional information about the dataset used in the paper can be found [here](http://personal.lse.ac.uk/tenreyro/lgw.html).    \n",
    "\n",
    "In this notebook we will use the Santos Silva and Tenreyro (2006) dataset to illustrate our results on estimation and inference of dyadic regression models. The dataset is located in the \"data\" directory of the Github repository. You'll need to download the data and adjust the **data** directory defined below to run this notebook on your computer.\n",
    "\n",
    "**References**    \n",
    "\n",
    "Santos Silva, J.M.C. and Tenreyro, Silvana. (2006). \"The Log of Gravity\", _Review of Economics and Statistics_ 88 (4): 641 - 658."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Python to plot all figures inline (i.e., not in a separate window)\n",
    "%matplotlib inline\n",
    "\n",
    "# Main scientific computing modules\n",
    "# Load library dependencies\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "# Import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# networkx module for the analysis of network data\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **dyadic_regression()** function in the _netrics_ package. This package has dependencies with the _ipt_ package. We therefore add the directories with these two codebases to our system path and then load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append location of ipt module base directory to system path\n",
    "# NOTE: only required if permanent install not made (see comments above)\n",
    "import sys\n",
    "sys.path.append('../../../ipt/')\n",
    "sys.path.append('../../../netrics/')\n",
    "\n",
    "# Load ipt and netrics modules\n",
    "import ipt as ipt\n",
    "import netrics as netrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of Santos Silva and Tenreyro (2006) replication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where graphics files will be saved (and where example data are stored)\n",
    "data =     '../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset into a pandas dataframe. Set (dyadic) multi-index and sort. Print out first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Log of Gravity.dta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1e5fb225e194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLogOfGravity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log of Gravity.dta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mLogOfGravity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's1_im'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's2_ex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# Set dataframe multi-index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mLogOfGravity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m's1_im'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's2_ex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLogOfGravity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py\u001b[0m in \u001b[0;36mread_stata\u001b[1;34m(filepath_or_buffer, convert_dates, convert_categoricals, encoding, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, iterator)\u001b[0m\n\u001b[0;32m    184\u001b[0m                          \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                          \u001b[0morder_categoricals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder_categoricals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                          chunksize=chunksize)\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buf, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, encoding, chunksize)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m             \u001b[1;31m# Copy to BytesIO, and ensure no encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Log of Gravity.dta'"
     ]
    }
   ],
   "source": [
    "LogOfGravity = pd.read_stata(data+\"Log of Gravity.dta\")\n",
    "LogOfGravity.set_index(['s1_im', 's2_ex'], drop = False, inplace = True)          # Set dataframe multi-index\n",
    "LogOfGravity.sort_index(level = ['s1_im', 's2_ex'], inplace = True)\n",
    "LogOfGravity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **trade** variable (*Y*) measures trade from exporter *i* to importer *j* in thousands of US dollars. Dividing by 1,000 scales the variable better for estimation. The set of covariates included in *W* correspond to those in used in Table 3 of Santos Silva and Tenreyro (2006, p. 650). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = LogOfGravity['trade'].copy(deep=True)/1000\n",
    "W = LogOfGravity[['lypex', 'lypim', 'lyex', 'lyim', 'ldist', 'border', 'comlang', 'colony', \\\n",
    "                  'landl_ex', 'landl_im', 'lremot_ex', 'lremot_im', 'comfrt_wto', 'open_wto']].copy(deep=True)\n",
    "W['constant'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dyadic_regression()** function fits simple linear, logistic and poisson regression models to dyadic data by the method of (dyadic or pairwise) composite maximum likelihood estimation. The function can accommodate directed outcomes, in which case a total of N(N-1) directed outcomes are observed (with N the number of agents), as well as undirected outcomes, in which case 0.5N(N-1) outcomes are observed. The program does not, as of yet, accommodate the presence of missing outcomes.    \n",
    "\n",
    "The function allows for three methods of standard error construction. All three of these methods are described in the forthcoming _Handbook of Econometrics_ chapter on networks by Graham (forthcoming). The first method assumes independence (conditional on covariates) across dyads. In the case of directed outcome data this does allow for dependence of $ Y_{ij} $ with $ Y_{ji} $ but rules out dependence between, say, $ Y_{ij} $ with $ Y_{ik} $. The other two estimators allow for dependence across any two dyads sharing at least one agent in common. The first of these corresponds to the Jackknife estimate of the leading term in the asymptotic variance of $ \\sqrt{N}\\left(\\hat{\\theta}-\\theta_{0}\\right) $. The second estimate is a bias corrected variance estimate which also includes (estimates of) higher order variance terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(netrics.dyadic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by attempting to replicate column 6 of Table 3 in Santos Silva and Tenreyro (2006, p. 650). The point estimates reported below agree with the ones reported in their table up to the three decimal places reported (with the exception of a few cases). Note the summary statistics computed using the downloaded replication file differ very slightly from those reported in Table A5 of the paper. This, along with optimization errors, likely account for the small discrepancies.    \n",
    "\n",
    "The standard errors reported below differ from those in the paper. This is because the paper uses a basic heteroscedastic robust variance-covariance estimate (see p. 645). This variance estimator requires independence of $ Y_{ij} $ and $ Y_{ji} $. In subsequent work Santos Silva and Tenreyro (2010, _Annual Review of Economics_) instead \"cluster\" on dyads; this corresponds to choosing the 'ind' option for variance estimation in the **dyadic_regression()** command. Of course neither of these two variance estimators make particularly plausible assumptions about the dependence structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                     silent=False, cov='ind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we repeat estimation, but this time compute standard errors using the bias corrected variance estimate mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                     silent=False, cov='DR_bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some covariates the increase in the standard error associated with allowing for dependence across dyads with agents in common is very small. But in some cases the standard errors increase quite a bit. Consider *comfrt_wto*, which is a dummy variable equal to 1 if the two trading partners are party to a (common) preferential trading agreement and zero otherwise. The standard error on this coefficient increases by almost 50 percent. Furthermore this is an important trade policy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "Next we conduct a short Monte Carlo experiment to illustrate the properties of inference methods based on the different variance-covariance estimates available in **dyad_regression()**. We set $ N = 100 $ and generate outcome data for all $N(N-1)$ ordered pairs of agents according to the outcome model:\n",
    "\n",
    "$$ Y_{ij}=\\exp\\left(\\theta_{1}W_{ij}+\\theta_{2}X_{i}+\\theta_{2}X_{j}\\right)A_{i}A_{j}U_{ij} $$\n",
    "\n",
    "Here $A_{i}$, for $i=1,...,N$, is a sequence of iid log normal random variables, each with mean 1 and scale parameter $ \\sigma_{A} $; $U_{ij}$ for $i=1,...,n$ with $n=N(N-1)$ is also sequence of iid log normal random variables, each with mean 1 and scale parameter $ \\sigma $.    \n",
    "\n",
    "Each agent is uniformaly at random assigned a location on the unit square, $W_{ij}$ equals the distance between agents *i* and *j* on that square; $X_{i}$ is a standard uniform random variable.    \n",
    "\n",
    "We set $\\theta_{1}=-1$, $\\theta_{1}=-1/2$ and $\\theta_{3}=1/2$. We set $ \\sigma = 1$ and $ \\sigma_{A} = 1/4$. This generates moderate, but meaningful, dependence across any two dyads sharing at least one agent in common.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "sigmaU = 1\n",
    "sigmaA = 1/4\n",
    "eta = [-1, -1/2, 1/2]\n",
    "\n",
    "B = 1000\n",
    "\n",
    "theta    = np.zeros((B,3))\n",
    "coverage = np.zeros((B,9))\n",
    "se       = np.zeros((B,9))\n",
    "\n",
    "\n",
    "for b in range(0,B):\n",
    "    start = time.time()\n",
    "    \n",
    "    A = np.random.lognormal(-sigmaA/2, sigmaA, N)\n",
    "    X1 = np.random.uniform(0, 1, N)\n",
    "    X2 = np.random.uniform(0, 1, N)\n",
    "    X3 = np.random.uniform(0, 1, N)\n",
    "\n",
    "    D = []\n",
    "    W = []\n",
    "    for i, j in it.permutations(range(0,N), 2):\n",
    "        W_ij = ((X1[i] - X1[j])**2 + (X2[i] - X2[j])**2)**(1/2)\n",
    "        Y_ij = (np.exp(W_ij*eta[0] + X3[i]*eta[1] + X3[j]*eta[2]))*A[i]*A[j]*np.random.lognormal(-sigmaU/2, sigmaU)\n",
    "        W.append([Y_ij, W_ij, X3[i], X3[j], i, j])\n",
    "    \n",
    "\n",
    "    W = pd.DataFrame(W, columns=['Y_ij', 'W_ij', 'X3_i', 'X3_j', 'i', 'j'])    \n",
    "    W = W.set_index(['i', 'j'], drop = True)  # Set dataframe multi-index\n",
    "    W['constant'] = 1\n",
    "\n",
    "    # Get outcome variable and drop from regressor matrix\n",
    "    Y = W['Y_ij'].copy(deep=True)\n",
    "    W.drop('Y_ij', axis=1, inplace=True)\n",
    "    \n",
    "    # Standard errors based on independence across dyads\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='ind')\n",
    "    \n",
    "    theta[b,:]    = theta_DR[0:3,:].T\n",
    "    \n",
    "    # Coverage\n",
    "    coverage[b,0] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,1] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,2] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "           \n",
    "    # Standard error length\n",
    "    se[b,0]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,1]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,2]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "    \n",
    "    \n",
    "    # Jackknife standard errors\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='DR')\n",
    "    \n",
    "    coverage[b,3] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,4] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,5] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "    \n",
    "    # Standard error length\n",
    "    se[b,3]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,4]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,5]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "    \n",
    "    # Bias corrected standard errors\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='DR_bc')\n",
    "    \n",
    "    coverage[b,6] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,7] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,8] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "    \n",
    "    # Standard error length\n",
    "    se[b,6]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,7]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,8]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "        \n",
    "    end = time.time()\n",
    "    if (b+1) % 100 == 0:\n",
    "        print(\"Time required f/ MC rep  \" + str(b+1) + \" of \" + str(B) + \": \" + str(end-start))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table reports Monte Carlo coverage estimates for Wald type nominal 95 percent confidence intervals based on the three variance estimates available in **dyadic_regression()**. The first assumes independence across dyads ('ind'), the second is the jackknife estimate ('DR'), and the third a bias corrected estimate ('DR_bc'). These last two allow for dependence across dyads sharing at least one agent in common. The columns in the table correspond to the three parameters in the model, the rows to the three variance estimators.   \n",
    "\n",
    "The table shows that assuming independence across dyads results in considerable undercoverage. The jackknife estimate, as would be expected, is conservative. Finally the biased corrected estimate has actual coverage very close to nominal coverage.   \n",
    "\n",
    "The standard error associated with a Monte Carlo coverage estimate is $\\sqrt{\\alpha\\left(1-\\alpha\\right)/B}$. With $B = 1,000$ simulations and $\\alpha = 0.05$ this results in a standard error of approximately 0.007 or a 95 percent confidence interval of $[0.936, 0.964]$. Hence the 'ind' standard errors significantly undercover, while the 'DR' standard errors (sometimes) significantly overcover. The 'DR_bc' have actual coverage (almost) insignificantly different from target coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(coverage,axis=0).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at average standard error length. The 'DR_bc' standard errors, while appreciably large than those base bone the (incorrect) assumption of independence across dyads, are smaller than the jackknife estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(se,axis=0).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports an attractive notebook style from Github\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen('http://bit.ly/1Bf5Hft')\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
